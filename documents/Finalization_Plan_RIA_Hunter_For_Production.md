# Plan to Finalize RIA Hunter for Production

## Current Status Overview

* **Backend (RIA-Hunter):** The data pipeline is largely complete – SEC Form ADV data has been ingested, cleaned, and stored in Supabase (including the ria\_profiles and narratives tables). The backend Next.js app is deployed on Vercel and exposes a basic /api/ask endpoint, but this endpoint is still a **stub** (it returns a placeholder or simple results, not a true AI answer)[\[1\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/documents/refactor/SUBPLAN_3_OPUS_rag_api.md#L47-L52). Critical pieces like retrieval of relevant data and calling a language model are not yet implemented. The Supabase database is set up with pgVector for embeddings, and a script exists to generate embeddings for RIA narratives, but this has not been fully run yet (the narratives.embedding column is likely still empty for most entries)[\[2\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/scripts/embed_narratives.ts#L92-L100).

* **Frontend (RIA-Hunter-App):** The Next.js front-end is deployed (e.g. at ria-hunter.app) and can communicate with the backend. The UI is implemented to accept user queries and display results. It currently shows an "AI Answer" section and a list of source RIA profiles in response to queries. Loading states and error handling are in place. However, because the backend’s AI logic is incomplete, the answers being shown are not truly AI-generated or focused. For example, asking **“What is the largest RIA in Missouri?”** currently returns multiple firms (even some in California) rather than a single, specific answer – indicating that the query is not being interpreted or answered correctly by an AI model.

* **Integration:** Basic integration between front-end and back-end is confirmed. The front-end calls the back-end’s /api/ask via a POST request (using NEXT\_PUBLIC\_API\_URL), and CORS has been enabled so that the browser requests succeed[\[3\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/04_cors_support.md#L6-L14)[\[4\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/04_cors_support.md#L97-L101). This means the plumbing is in place, but the core logic to process queries and generate a proper answer is still missing.

## Outstanding Issues and Gaps

* **Incomplete AI Query Handling:** The /api/ask endpoint does not yet parse natural language questions or apply any intelligent filtering. It likely performs a trivial or no search, leading to irrelevant results. In particular, **no logic exists to handle location-based queries or superlatives** (e.g. identifying “Missouri” in the question and restricting results to that state, or understanding what “largest” means in context) – hence the Missouri query returning extraneous data.

* **No Language Model Integration:** The back-end is not yet calling any GenAI model to synthesize an answer. As a result, the “AI Answer” is either a hard-coded stub or a simplistic aggregation of data. There is **no actual natural-language generation happening**. The plan was to use a retrieval-augmented approach with a model like Google Vertex AI (Gemini)[\[1\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/documents/refactor/SUBPLAN_3_OPUS_rag_api.md#L47-L52), but this hasn’t been implemented. This is why the answers are not phrased as a single clear sentence or paragraph that directly addresses the question.

* **Embeddings and Semantic Search Not Utilized:** While the data (RIA narratives) exists, and pgVector is set up in the database, the app is not yet using vector similarity search to find relevant textual information. The scripts/embed\_narratives.ts job (which would generate embeddings for each RIA’s narrative using Vertex AI’s embedding model) has not been completed or its output not loaded into the database[\[2\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/scripts/embed_narratives.ts#L92-L100). Without embeddings, the system likely falls back to no semantic search or a basic keyword filter, limiting the quality of results for complex queries.

* **Query Result Ranking/Filtering:** There is no logic to rank or filter the RIA profiles before sending to the front-end aside from whatever basic query is done. For example, the query about “largest RIA in Missouri” should have filtered to Missouri firms and then identified the one with the maximum AUM. Currently, that logic is missing, leading to multiple source listings and no clear answer. Similarly, other query types (e.g. questions about a specific firm or specialty) might not be handled optimally without some query parsing or structured filtering.

* **Miscellaneous Known Bugs:** There is a known issue in the “Living Profile” feature where users can add duplicate tags (Bug \#002). This part of the application (user-added tags to RIA profiles) is separate from the core Q\&A search, so it doesn’t prevent launching the search functionality. However, it should be fixed before that feature goes live to production to ensure data consistency (e.g. disallow duplicate tag entries for a profile). Another historical bug (related to parsing a rare SEC form structure) was rendered moot by the move to the new pipeline, so it’s not a blocker now.

## Action Plan to Complete the Project

To get RIA Hunter across the finish line, we should prioritize implementing the core retrieval and answer-generation functionality, then address supporting tasks like data indexing and quality fixes. Below is a step-by-step plan:

**1\. Implement the Retrieval-Augmented /api/ask Endpoint (Backend):**  
Complete the backend logic so that for a given natural language query, the system gathers relevant data and produces an answer. According to the design, this involves several sub-steps: parsing the query, querying Supabase for matching adviser info (using SQL filters and/or vector similarity), formulating a prompt with the top results, calling a language model to get an answer, and returning the answer with source references[\[1\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/documents/refactor/SUBPLAN_3_OPUS_rag_api.md#L47-L52). Concretely:  
\- **Parse the Query:** Extract key terms or structured hints. For example, detect if the query contains a state name or other filters (more on this in step 3 below).  
\- **Retrieve Relevant RIAs:** Use the parsed info to query the database. This could combine a pgVector similarity search on narratives with additional filters. If embeddings are ready (see step 2), use a vector similarity query to find the most relevant RIA narratives. Otherwise, as an interim solution, you might perform a text search (ILIKE '%keyword%') or a simple filter by known fields. Ensure that if a location filter is intended (state, city) or other criterion (like AUM range), you apply those in the SQL query to narrow results. Retrieve the top N results (perhaps 5\) that best match the query intent.  
\- **Formulate the AI Prompt:** Take the retrieved data (e.g. a few key facts from each top RIA – name, location, AUM, and perhaps a snippet of their narrative or the specific fact that makes them relevant) and construct a prompt for the language model. The prompt should succinctly present the data and ask the model to answer the user’s question. For example: *“User asks: 'What is the largest RIA in Missouri?' Relevant data: Firm A – based in MO, AUM \\$5B; Firm B – based in MO, AUM \\$3B. Answer the question using these facts.”* This ensures the model focuses on the provided data.  
\- **Call the Generative Model:** Use a language model API to get a natural language answer. The plan suggests using **Google Vertex AI’s Generative models (Gemini)** via the Node SDK[\[1\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/documents/refactor/SUBPLAN_3_OPUS_rag_api.md#L47-L52). If Vertex AI access or model readiness is an issue, alternatively use OpenAI’s GPT-4 or GPT-3.5 via their API. Ensure the API keys and necessary credentials are configured (for Vertex, set GOOGLE\_PROJECT\_ID and auth; for OpenAI, an OPENAI\_API\_KEY). The backend should remain the only place that calls the AI API (the front-end just gets the final text), to keep keys secure.  
\- **Return Answer and Sources:** Structure the response JSON as { answer: "...

textanswer

...", sources: \[ ...list of source RIAs... \] }. The sources can include identifiers or data (like firm name, CRD, etc.) to allow the front-end to display them. The answer should be a concise paragraph or sentence directly addressing the question, synthesized from the data. For the Missouri query example, the answer could be: *“The largest RIA headquartered in Missouri is* *Firm A, with approximately \\$5 billion in assets under management.”* – and the sources list would contain Firm A (and perhaps a couple of the next ones as backup references).  
\- **Implement CORS/Options (if not already):** It appears CORS support was added[\[3\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/04_cors_support.md#L6-L14), but double-check that the final /api/ask implementation still handles the OPTIONS preflight and sets the Access-Control-Allow-Origin header (especially if the backend API is on a different domain from the frontend). This was addressed in a recent update, so just ensure it stays intact when modifying the route.

**2\. Generate and Index Narrative Embeddings:**  
To enable semantic search on the RIA narratives (so the AI can find relevant firms even when the query wording doesn’t exactly match the text), run the embedding generation process and store the vectors in Supabase. The script scripts/embed\_narratives.ts is already written to do this using Vertex AI’s embedding model (textembedding-gecko)[\[5\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/scripts/embed_narratives.ts#L36-L44). Steps to execute:  
\- Make sure the Supabase vector extension is enabled and the narratives.embedding column exists with the correct type (the script attempts this via an RPC call)[\[6\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/scripts/embed_narratives.ts#L60-L69)[\[7\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/scripts/embed_narratives.ts#L72-L75). In the production Supabase, you might need to manually run the SQL to add the embedding column if it’s not there or adjust its dimension to match the model (e.g. 768 dims for Gecko).  
\- Obtain credentials for the embedding model. If using Vertex AI, ensure the Google service account has access to the Generative AI API (the script uses Vertex AI Preview library). If using OpenAI embeddings as an alternative, you’d modify the script accordingly.  
\- Run the script (it can be executed locally or as a one-off in the deployment) to fill embeddings. It will batch through all RIA narratives, generate vector embeddings for each in batches, and update each row in the narratives table with the embedding[\[2\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/scripts/embed_narratives.ts#L92-L100)[\[8\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/scripts/embed_narratives.ts#L128-L136). For \~30k RIAs (if that’s the scale), this might take some time and incur API costs, but it’s typically a one-time (or infrequent) process.  
\- Once embeddings are stored, modify the retrieval logic in /api/ask (from step 1\) to use a **vector similarity search**. In practice, with Supabase you can use a SQL query with the \<-\> operator on the vector column to get nearest neighbors. For example: SELECT crd\_number, narrative FROM narratives ORDER BY embedding \<-\> query\_embedding LIMIT 5; (where query\_embedding is the vector for the user’s question, computed by the same embedding model). The Supabase JS client may allow calling a stored procedure for this, or you can use a .rpc() call or a raw SQL query through Supabase. This will yield the top relevant narratives. Then, use those results (joined with the ria\_profiles table to get firm names, etc.) for constructing the LLM prompt.  
\- *Interim alternative:* If generating embeddings for all entries is not immediately feasible, consider using a **hybrid search** approach. Supabase’s full-text search or simple filtering can be combined with vector search. For instance, you could first filter by state or other criteria, and then do a vector search on that subset or vice versa. In any case, the goal is to significantly improve relevance by more than just naive text matching.

**3\. Improve Query Parsing for Filters and Special Queries:**  
Enhance the backend query parsing to handle common patterns that require structured data operations, so the answers become more accurate:  
\- **Location-Based Queries:** If the user’s question contains a state name or abbreviation (e.g. “Missouri” or “MO”), the backend should recognize this and apply a filter on the state field of ria\_profiles. We can maintain a list of state names and their abbreviations to check against the query string. For example, a simple check like: for each state in statesList: if stateName or stateAbbrev appears in query (as a whole word) \-\> set locationFilter. When a state filter is set, use it in the Supabase query (e.g. .eq('state', 'MO')) so that we only retrieve RIAs from that state. This will prevent irrelevant out-of-state results from even being considered by the AI.  
\- **Superlatives and Ranking (e.g. "largest", "smallest"):** Identify words like "largest", "biggest", "smallest", "top 5", etc. These indicate the user is asking for an extreme or a ranking. For the “largest RIA in Missouri” example, once we know the state \= MO, and that "largest" implies highest AUM, we can directly query the database for the firm with max AUM in that subset. In fact, a straightforward approach is to bypass the AI for determining the facts: do SELECT \* FROM ria\_profiles WHERE state='MO' ORDER BY aum DESC LIMIT 1. This gives the largest firm in Missouri (let’s call it Firm X with $Y AUM). Then you can feed that fact into the LLM prompt (or even just directly compose the answer string without the LLM if you prefer absolute precision for this case). However, using the LLM can help format the answer nicely and include any additional context (“as of the latest filings, Firm X in St. Louis, MO manages the largest assets…”). The key is that the selection of sources to feed the LLM should be constrained correctly — e.g. only Firm X, or perhaps Firm X plus a couple of others for comparison if needed.  
\- **Direct Facts Queries:** Similarly, if the question asks for a specific known fact or count (e.g. “How many RIAs are there in California?” or “Who is ABC Wealth’s chief compliance officer?”), consider handling these via the data directly. The narrative texts might not contain some facts like counts, but the structured data could answer them. For a count question, you might not even need the LLM (you can just return the number). For now, focusing on location and "largest/smallest" covers the most obvious tricky query.  
\- **Fallback to Semantic Search for General Queries:** For questions that are not easily answered by a direct database query (e.g. “Which RIAs specialize in biotech investments?” or “Tell me about XYZ Advisors’ business focus”), rely on the vector semantic search. The narratives likely contain those details, and the LLM can summarize them. Just ensure that if a query includes a proper noun (like a firm name), you detect that and perhaps treat it as a direct search on legal\_name as well, so that the firm’s own profile is definitely included in sources.  
\- Implementing this parsing doesn’t need an overly complex NLP pipeline – a few well-chosen heuristics and checks can go a long way for the types of queries expected. Over time, if you gather more query data, you could refine this or train a small classification model, but for now a rule-based approach is sufficient and faster to implement.

**4\. Integrate and Configure the AI Model for Answer Generation:**  
With relevant data in hand for a query, the final piece is to turn that into a useful answer. Ensure the backend is set up to call an AI model:  
\- If using **Google Vertex AI (Generative)**: You’ll use the Vertex AI Node.js library (already included) to call a model like text-bison@001 for text completion or a chat model. Make sure the service account JSON is provided (possibly set GOOGLE\_APPLICATION\_CREDENTIALS on Vercel if needed, or use ADC if running in GCP environment) and GOOGLE\_PROJECT\_ID is set[\[9\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/documents/refactor/SUBPLAN_3_OPUS_rag_api.md#L69-L72). Construct the prompt carefully to include the context and possibly some instruction (e.g. “Answer concisely and cite the firm names.”). Then call the predict() method similar to how the embedding was done, but using the text generation model. Parse out the model’s response text.  
\- If using **OpenAI API** as an alternative: Include the OpenAI SDK, set OPENAI\_API\_KEY in the env. Use the Chat Completion API with a system message that the assistant should answer based on given data, and a user message containing the question and formatted context. This might even be quicker to implement initially.  
\- **Testing the Output:** Whichever model, try a few queries in a dev environment to see how it responds. You may need to adjust the prompt format or give examples. For instance, ensure the model doesn’t hallucinate extra facts – emphasize that it should only use the provided data. Given that the answers are relatively fact-based (like names and numbers from the profile), a well-scoped prompt should suffice. If the model tends to list multiple firms but you want just one, instruct it accordingly (e.g. “If asked for ‘the largest’, only name the single largest firm.”).  
\- **Performance Consideration:** Calling an LLM for each query will add latency. This is expected, but you might consider small optimizations: e.g., if no relevant data is found, skip the LLM call and return a “No information found” message directly; or cache results for frequent queries. These are secondary, though – first get it working correctly.

**5\. Thorough Testing and Refinement:**  
Once the above is implemented, test the end-to-end behavior and make adjustments:  
\- **Example Queries:** Verify that the specific issue is resolved: asking “What is the largest RIA in Missouri?” should return a single clear answer identifying the firm and its AUM, with that firm listed as a source (and it should indeed be a Missouri firm). Also test variations: “largest RIA in MO” (abbr.), “biggest RIA in California”, “smallest RIA in Texas”, etc., to ensure your parsing logic catches them.  
\- Test general queries like “What does Firm X do?” or “Which RIAs focus on sustainable investing?” and see if the answers make sense and sources correlate. If the AI answer is off, check if the retrieval pulled the wrong data. This might lead you to tweak the number of sources or the wording of the prompt given to the model. For example, you might find you need to provide a bit more detail in the context for the model to answer accurately.  
\- **Edge Cases:** Try queries with no good answer in the data (“Who is the President of the United States?” – which is outside the domain) to ensure the system handles it gracefully (perhaps the model will say it’s outside scope, or you can catch it if retrieval comes up empty and return a polite “I don’t have information on that”). Also, test the error handling by turning off network or using invalid API keys to see that your frontend displays the error message nicely (the front-end’s error UI is in place already).  
\- **Frontend Display:** Ensure that the front-end correctly shows the AI answer and sources. With real answers coming in, verify formatting (e.g. if the answer is long, does it overflow nicely? Do source names appear as expected?). The current UI uses a collapsible "Sources" section with count – confirm that works with the actual source list returned.  
\- **Iteration:** Based on testing, refine the backend. You might adjust how many sources to include. For very direct queries (like the “largest” example), including just the top 1 source might be best. For more open queries, maybe top 3-5 sources give the model enough to chew on. Also consider the length of narrative to include – you might truncate or summarize narratives if they’re long, to fit within token limits of the model.

**6\. Deployment and Environment Setup:**  
After testing locally or in a staging environment, deploy the updated backend to Vercel (and the frontend if any changes were made there). A few things to double-check for a smooth production rollout:  
\- **Environment Variables:** Make sure all required API keys and configs are set in Vercel’s environment settings for the backend. This includes the Supabase URL and Service Role key (for data access), the AI model credentials (Google or OpenAI keys), and any CORS origin or other config. The plan notes that missing env vars can cause build issues, especially for Edge functions[\[9\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/documents/refactor/SUBPLAN_3_OPUS_rag_api.md#L69-L72), so verify that Vercel has everything and the build logs show no warnings. Also ensure that NEXT\_PUBLIC\_API\_URL is correctly set in the front-end deployment to point to the backend’s URL.  
\- **Database State:** Confirm the Supabase database is fully seeded with the latest data. If you ran the embedding generation locally, those embeddings should now reside in the cloud DB (if you pointed the script to it). If not, run the embedding process against the production DB. Also, confirm the ria\_profiles and narratives tables have the expected row counts (to ensure no data was missed).  
\- **Migration Scripts:** If any schema changes were made (e.g. adding the embedding column or creating any stored procedures for vector search), apply those migrations to the Supabase instance. The repository’s supabase/migrations folder or seed scripts should reflect any changes needed. Running schema.sql (which creates tables and extension[\[10\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/seed/schema.sql#L7-L15)[\[11\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/seed/schema.sql#L18-L22)) on the production DB was likely done initially, but do apply any new updates.  
\- **Monitoring:** Once live, monitor the application. Given this is the first time the AI features are live, watch the logs (Vercel function logs or Supabase logs) for errors or slow queries. It might be wise to add basic logging around the /api/ask handler to record query timings and any exceptions. This can help catch issues like timeouts from the AI API or bottlenecks in the DB. Since ranking and selection of sources is done on the fly, you might log which sources were returned for a query, so you can later analyze if they were reasonable.

**7\. Address Remaining Polish Items:**  
With the core Q\&A functionality working, you can turn attention to secondary features and fixes:  
\- **Fix “Living Profile” Tag Duplication:** Implement a check in the endpoint or front-end where users add tags to a profile, to prevent duplicates. This could be as simple as a query that checks existing tags for that RIA before inserting a new one, and returning an error or ignoring the request if it’s a duplicate. Ensuring uniqueness will improve the quality of that feature when it’s exposed to users.  
\- **UI/UX Enhancements:** Review the UI now that real answers are showing. Possibly, you might want to highlight the state or AUM in the source list, or link the RIA names to a detailed profile page (if those exist in the app). Make sure mobile view is acceptable. These tweaks can elevate the user experience but are not blockers for functionality.  
\- **Investment Thesis Matcher:** This is a planned feature (matching user-entered text to RIA profiles). It likely relies on the same embeddings and semantic search capability now being put in place. After launch of the core search, this could be the next big feature. It might involve allowing a user to input their “thesis” and then doing a similar vector search among RIA profiles’ strategies. Keep this in mind, as the work done now (embeddings, vector queries, LLM integration) will directly support building that feature.  
\- **Ongoing Data Updates:** Plan how new RIA data or updates to AUM will be ingested moving forward. The pipeline to fetch new Form ADV filings via Document AI should be run periodically. Ensure that running it and re-embedding new narratives is straightforward (perhaps create a cron job or a manual trigger to update data every quarter or so). This ensures the product’s answers stay up-to-date over time.

By executing the steps above, we will go from a nearly-there prototype to a production-ready RIA Hunter application. The primary goal is to enable the system to **understand queries and respond with a single, well-supported answer**. Once the /api/ask endpoint properly leverages the data and an AI model, the example query about the largest RIA in Missouri (and other queries) will yield the correct, concise result – which is the kind of experience that will meet user expectations for a GenAI-powered product.

Finally, after deploying these changes, conduct a final review with end-users or stakeholders to gather feedback. This will confirm that the solution is not only technically sound but also delivering value in terms of answer relevance and usability. With that feedback, you can iterate further, but at this point RIA Hunter will have crossed the finish line into a viable product. Good luck\! 🚀

---

[\[1\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/documents/refactor/SUBPLAN_3_OPUS_rag_api.md#L47-L52) [\[9\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/documents/refactor/SUBPLAN_3_OPUS_rag_api.md#L69-L72) SUBPLAN\_3\_OPUS\_rag\_api.md

[https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/documents/refactor/SUBPLAN\_3\_OPUS\_rag\_api.md](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/documents/refactor/SUBPLAN_3_OPUS_rag_api.md)

[\[2\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/scripts/embed_narratives.ts#L92-L100) [\[5\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/scripts/embed_narratives.ts#L36-L44) [\[6\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/scripts/embed_narratives.ts#L60-L69) [\[7\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/scripts/embed_narratives.ts#L72-L75) [\[8\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/scripts/embed_narratives.ts#L128-L136) embed\_narratives.ts

[https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/scripts/embed\_narratives.ts](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/scripts/embed_narratives.ts)

[\[3\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/04_cors_support.md#L6-L14) [\[4\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/04_cors_support.md#L97-L101) 04\_cors\_support.md

[https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/04\_cors\_support.md](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/04_cors_support.md)

[\[10\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/seed/schema.sql#L7-L15) [\[11\]](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/seed/schema.sql#L18-L22) schema.sql

[https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/seed/schema.sql](https://github.com/Turnstyle/ria-hunter/blob/b6379288fe96033ce1981229283050bce5130d71/seed/schema.sql)